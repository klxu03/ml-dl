{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 - Basic Bayes\n",
    "You have seen a demostration of Bayesian inference in section. Your homework will explore simple variation of it to solidify your understanding of priors, likelihoods, and posteriors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro: Poisson Likelihood + Gamma Prior -> Gamma Posterior\n",
    "### Setup\n",
    "Assume we have $X1, X_2, \\dots, X_n$ independent and identically distributed (i.i.d) Poisson distribution. So, \n",
    "\n",
    "$$X_i \\sim \\text{Pois}(\\lambda) \\text{ for all } i.$$\n",
    "\n",
    "You can imagine $x_i$ as counting the number of telephone calls in day $i$, which follows a Poisson distribution, where $\\lambda$ is the (unknown) average number of phone calls a day.\n",
    "\n",
    "**Goal:** We want to conduct Bayesian inference on the data $x1, \\dots, x_n$ in order to infer the unknown parameter $\\lambda$.\n",
    "\n",
    "### a. Likelihood $p(x | \\lambda)$\n",
    "The pmf of Poisson is \n",
    "\n",
    "$$p(x | \\lambda) = \\frac{\\lambda^{x}e^{-\\lambda}}{x!}.$$\n",
    "\n",
    "Therefore, by i.i.d. assumption, the joint likelihood of all the $n$ pieces of data will be the product of the pmf, simplified for you here\n",
    "$$p (x1, \\dots, x_n) = \\frac{\\lambda^{x1 + \\dots  + x_n} e^{-n\\lambda}}{x1! \\dots x_n!}.$$\n",
    "\n",
    "### b. Prior $p(\\lambda)$\n",
    "Remember, we don't know what $\\lambda$ is, so we will treat it as a random variable $\\Lambda$ (this is capital letter for $\\lambda$). Magically, if we let $\\Lambda$ follow a Gamma distribution, we get a nice posterior, so we will do just that. Now, the pdf of gamma(shape=$\\alpha$, rate=$\\beta$) is\n",
    "\n",
    "$$p(\\lambda) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}e^{-\\beta\\lambda},$$\n",
    "\n",
    "where $\\Gamma(\\cdot)$ is the gamma function. When you choose your prior, **choose any $\\alpha >0, \\beta>0$ that is suitable for the prior knowledge you have about the data**. See section notes if you feel unfamiliar about it.\n",
    "\n",
    "_Do not be intimidated by this crazy formula! It will be very friendly to us in the end of the calculation. Question: doesn't the gamma distribution look kind of similar to the Poisson distribution? This may give us a sense that the posterior will be nice! :)_\n",
    "\n",
    "### c. Posterior $p(\\lambda | x)$\n",
    "We're almost done. \n",
    "\n",
    "Recall: The formula of the posterior distribution is\n",
    "$$\n",
    "p(\\lambda | x) = \\frac{p(x|\\lambda)p(\\lambda)}{\\int_{\\lambda}p(x|\\lambda)p(\\lambda)d\\lambda} = \\frac{\\text{likelihood} \\cdot \\text{prior}}{\\text{normalizing constant}}.\n",
    "$$\n",
    "We will skip the algebra and just tell you that \n",
    "$$\n",
    "p(\\lambda | x) = \\frac{\\lambda^{{\\color{blue}{x1 + \\dots + x_n + \\alpha}} - 1} e^{-({\\color{blue}{n + \\beta}})\\lambda}}{\\text{normalizing constant}}.\n",
    "$$\n",
    "The normalizing constant is not very important, the **MAIN TAKEAWAY IS THAT THE POSTERIOR IS ALSO DISTRIBUTED GAMMA! :)** In fact, the posterior is \n",
    "$$\n",
    "\\text{gamma}(x1 + \\dots + x_n + \\alpha, n + \\beta).\n",
    "$$\n",
    "\n",
    "### Summary\n",
    "- Likelihood $p(x | \\lambda) \\sim \\text{Pois}(\\lambda)$\n",
    "- Prior $p(\\lambda) \\sim \\text{gamma}(\\alpha, \\beta)$\n",
    "- Posterior $p(\\lambda | x)\\sim \\text{gamma}(x1 + \\dots + x_n + \\alpha, n + \\beta)$\n",
    "\n",
    "With this, you are ready for this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Data\n",
    "Run the following two code cells.\n",
    "\n",
    "You will perform bayesian analysis on two different sets of data and using two different priors, for a total of 4 cases.\n",
    "\n",
    "Let the full data ($n=30$) be\n",
    "$$\n",
    "x = [ 8,  8,  7, 11, 10,  6,  7, 11,  5, 12,  8,  7,  8,  8, 11,  4,  3,\n",
    "        9,  9,  4,  7,  7,  9, 12,  8,  9, 10,  9,  8,  8]\n",
    "$$\n",
    "Define:\n",
    "- ```x```, the original 30 data points\n",
    "- ```x_short```, only the first 3 data points\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [ 8,  8,  7, 11, 10,  6,  7, 11,  5, 12,  8,  7,  8,  8, 11,  4,  3,  9,  9,  4,  7,  7,  9, 12,  8,  9, 10,  9,  8,  8]\n",
    "n = len(x)\n",
    "print('n:', n, '\\nmean:', np.mean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_short = x[:3]\n",
    "n_short = len(x_short)\n",
    "print('x_short:', x_short)\n",
    "print('n_short:', n_short, '\\nmean x_short:', np.round(np.mean(x_short),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Peaked Prior\n",
    "\n",
    "Recall the story about inferring the average number of phone calls. First, we want to create a (peaked) prior that reflects our belief about what the data (number of phone calls) is.     \n",
    "\n",
    "Suppose Mrs. Morgan said, \"From my experience and memory, I think the average number of phone calls every day is 4. Most of the time (like 95% of the time), it's between 2 to 6 calls every day.\"\n",
    "\n",
    "## 1.1 Prior\n",
    "**Task:**\n",
    "1. [2 pt] Define appropriate ```alpha_prior1```, ```beta_prior1``` to obtain a suitable prior gamma($\\alpha, \\beta$) with appropriate mean and variance. \n",
    "\n",
    "    (Hint: for gamma($\\alpha, \\beta$)\n",
    "    - mean = $\\frac{\\alpha}{\\beta}$ and variance = $\\frac{\\alpha}{\\beta^2}$. \n",
    "    - It may also be easier to define $\\beta$ before defining $\\alpha$.)\n",
    "2. [1 pt] Compute ```prior1```, which is the pdf of gamma($\\alpha, \\beta$) along the $\\lambda$-axis `lamb`. \n",
    "    - Make sure you read the scipy documentation correctly and input the correct arguments (rate and scale are reciprocals of each other!).\n",
    "3. [1 pt] Plot the density of the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO alpha and beta\n",
    "beta_prior1 = None\n",
    "alpha_prior1 = None\n",
    "\n",
    "# TODO prior1\n",
    "lamb = np.linspace(0, 15, 1000)\n",
    "prior1 = None\n",
    "\n",
    "# TODO plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Posterior parameters\n",
    "Let the posterior parameters be\n",
    "- ```alpha_post1```, be the posterior shape for full data ```x```.\n",
    "- ```beta_post1```, be the posterior rate for full data ```x```.\n",
    "- ```alpha_post1_short```, be the posterior shape for short data ```x_short```.\n",
    "- ```beta_post1_short```, be the posterior rate for short data ```x_short```.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "[2 pt] Define ```alpha_post1```, ```beta_post1```, ```alpha_post1_short```, and ```beta_post1_short``` using the formula in the introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO alpha, beta\n",
    "alpha_post1 = None\n",
    "beta_post1 = None\n",
    "\n",
    "alpha_post1_short = None\n",
    "beta_post1_short = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Posterior plot\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1. [2 pt] Define ```posterior1``` and ```posterior1_short```, the respective pdf of the posteriors. \n",
    "    - Use the same horizontal axis ```lamb``` from previous parts.\n",
    "2. [1 pt] Plot the three densities (prior1, posterior1, posterior1_short) on the same figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO posterior densities\n",
    "posterior1 = None\n",
    "posterior1_short = None\n",
    "\n",
    "# TODO plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Peaked MAP\n",
    "**Task:**\n",
    "\n",
    "1. [1 pt] Compute the MAP estimator for both posteriors, storing it as ```lamb_MAP1``` and ```lamb_MAP1_short```. \n",
    "\n",
    "2. [1 pt] Print both these values, rounded to 3 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO MAP\n",
    "lamb_MAP1 = None\n",
    "lamb_MAP1_short = None\n",
    "\n",
    "# TODO print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Discuss MAP\n",
    "[2 pt] What do you observe about the MAP estimator for the full data and short data? Which is \"closer\" to the prior? Give an explanation for your observations.\n",
    "\n",
    "**Ans:** type answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Flat Prior\n",
    "Now, compare the peaked prior to the next procedure, which assumes a prior that is very neutral, i.e., flat prior.\n",
    "\n",
    "But we want to the prior to still be gamma, so we choose parameters accordingly to look essentially flat.\n",
    "\n",
    "## 2.1 Prior\n",
    "**Task:**\n",
    "1. [2 pt] Define appropriate ```alpha_prior2```, ```beta_prior2``` to obtain a prior gamma($\\alpha, \\beta$) with mean 15 and variance 100. \n",
    "2. [1 pt] Compute ```prior2```, which is the pdf of gamma($\\alpha, \\beta$). \n",
    "    - Make sure you read the scipy documentation correctly and input the correct arguments (rate and scale are reciprocals of each other!). \n",
    "    - Use the same ```lamb``` as before.\n",
    "3. [1 pt] Plot the density of the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO alpha, beta\n",
    "beta_prior2 = None\n",
    "alpha_prior2 = None\n",
    "\n",
    "# TODO prior\n",
    "prior2 = None\n",
    "\n",
    "# TODO plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Posterior parameters\n",
    "Like before, you will perform bayesian analysis on ```x``` and ```x_short```.\n",
    "\n",
    "Let the posterior parameters be\n",
    "- ```alpha_post2```, be the posterior shape for full data ```x```.\n",
    "- ```beta_post2```, be the posterior rate for full data ```x```.\n",
    "- ```alpha_post2_short```, be the posterior shape for short data ```x_short```.\n",
    "- ```beta_post2_short```, be the posterior rate for short data ```x_short```.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "[2 pt] Define ```alpha_post2```, ```beta_post2```, ```alpha_post2_short```, and ```beta_post2_short``` using the formula in the introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO alpha, beta\n",
    "alpha_post2 = None\n",
    "beta_post2 = None\n",
    "\n",
    "alpha_post2_short = None\n",
    "beta_post2_short = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Posterior plot\n",
    "**Task:**\n",
    "\n",
    "1. [2 pt] Define ```posterior2``` and ```posterior2_short```, the respective pdf of the posteriors. \n",
    "    - Use the same horizontal axis ```lamb``` from previous parts.\n",
    "2. [1 pt] Plot the three densities (prior2, posterior2, posterior2_short) on the same figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO posterior\n",
    "posterior2 = None\n",
    "posterior2_short = None\n",
    "\n",
    "# TODO plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Flat MAP\n",
    "**Task:**\n",
    "\n",
    "1. [1 pt] Compute the MAP estimator for both posteriors, storing it as ```lamb_MAP2``` and ```lamb_MAP2_short```. \n",
    "\n",
    "2. [1 pt] Print both these values, rounded to 3 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO MAP\n",
    "lamb_MAP2 = None\n",
    "lamb_MAP2_short = None\n",
    "\n",
    "# TODO print\n",
    "print('lamb_MAP_flat      :', np.round(lamb_MAP2,3))\n",
    "print('lamb_MAP_flat_short:', np.round(lamb_MAP2_short,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Compare choice of prior with full data (n=30)\n",
    "Run the cell below and produce the correct plots. You should not need to change any code here.\n",
    "\n",
    "[1 pt] With sufficiently large amount of data, does it seem to matter very much what the prior is? What difference between MAP estimators for peaked and flat priors do you expect when even more data is used for computing the posteriors (e.g., $n$=50, 100, etc.)?\n",
    "\n",
    "**Ans:** type answer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP\n",
    "print('lamb_MAP1  :', np.round(lamb_MAP1,3))\n",
    "print('lamb_MAP2  :', np.round(lamb_MAP2,3))\n",
    "print('true lambda: 8.000')\n",
    "\n",
    "# plot\n",
    "plt.plot(lamb, prior1, 'b--', linewidth=3, label='prior')\n",
    "plt.plot(lamb, posterior1, 'b', linewidth=3, label='posterior peaked')\n",
    "plt.plot(lamb, prior2, 'g--', linewidth=3, label='prior')\n",
    "plt.plot(lamb, posterior2, 'g', linewidth=3,label='posterior flat')\n",
    "plt.title(f'Comparing peaked prior and flat prior'); \n",
    "plt.xlabel(r'$\\lambda$'); plt.ylabel(r'Density')\n",
    "plt.grid(alpha=.4, linestyle='--'); plt.legend()\n",
    "plt.ylim([0,1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
