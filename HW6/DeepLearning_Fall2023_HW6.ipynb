{"cells":[{"cell_type":"markdown","metadata":{"id":"ZQCz6Qbn8Xi3"},"source":["\n","# Deep Learning Homework 6 (Spring 2023)\n","\n","This code is provided for Deep Learning class (601.482/682) Homework 6. For ease of implementation, we recommend working entire in Google Colaboratory.\n","\n","@Copyright Cong Gao, the Johns Hopkins University, cgao11@jhu.edu. Modifications made by Hongtao Wu, Suzanna Sia, Hao Ding, Keith Harrigian, and Yiqing Shen.\n"]},{"cell_type":"markdown","metadata":{"id":"AGA2oroEWDk1"},"source":["### Imports"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"WyxXeYArKAIh"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounting Failed.\n"]}],"source":["## Mount Google Drive Data (If using Google Colaboratory)\n","try:\n","    from google.colab import drive\n","    drive.mount('/content/gdrive')\n","except:\n","    print(\"Mounting Failed.\")"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"diX3FqyIWDk2"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["## Standard Library\n","import os\n","import json\n","\n","## External Libraries\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torchvision import transforms\n","from torch.autograd import Variable\n","import torch.nn.functional as functional\n","from torch.utils.data import Dataset, DataLoader\n","from skimage import io\n","import matplotlib.pyplot as plt\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"Byv0ZVSoWDk2"},"source":["# Problem 1: Unsupervised Pre-training"]},{"cell_type":"markdown","metadata":{"id":"x4NCcExeWDk2"},"source":["### Training Hyperparameters\n","\n","These are recommended hyperparameters - please feel free to use what works for you. Batch size can be changed if it does not match your memory, please state your batch step_size in your report."]},{"cell_type":"markdown","metadata":{"id":"OGb5kRvG1dcD"},"source":["Dataset is available at: https://livejohnshopkins-my.sharepoint.com/:u:/g/personal/yshen92_jh_edu/EcTxWAXsAhtDiv3vUxCTF8gBgAARCUvvKthb3s-pEExyMg"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"sl_S6MdMWDk2"},"outputs":[],"source":["## Batch Size\n","train_batch_size = 10\n","validation_batch_size = 10\n","\n","## Learning Rate\n","learning_rate = 0.001\n","\n","# Epochs (Consider setting high and implementing early stopping)\n","num_epochs = 200"]},{"cell_type":"markdown","metadata":{"id":"m3BRbLhNWDk3"},"source":["### Data Paths"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"aGfa8qhNWDk3"},"outputs":[],"source":["# General Data Directory ##TODO: Please fill in the appropriate directory\n","data_dir = \"./HW6_data\"\n","\n","## Segmentation + Colorization Paths\n","segmentation_data_dir = f\"{data_dir}/segmentation/\"\n","colorization_data_dir = f\"{data_dir}/colorization/\"\n","\n","# Mask JSON\n","mask_json = f\"{data_dir}/mapping.json\""]},{"cell_type":"markdown","metadata":{"id":"gpAXntx2WDk3"},"source":["### Data Loaders\n","\n","We have provided you with some preprocessing code for the images but you should feel free to modify the class however you please to support your training schema. In the very least, you will have to modify the dataloader to support loading of the colorization dataset."]},{"cell_type":"code","execution_count":39,"metadata":{"id":"gL-hqHd1WDk4"},"outputs":[],"source":["## Image Transforms\n","img_transform = transforms.Compose([\n","        transforms.ToTensor(),\n","])\n","\n","## Image Dataloader\n","class ImageDataset(Dataset):\n","\n","    \"\"\"\n","    ImageDataset\n","    \"\"\"\n","\n","    def __init__(self,\n","                 input_dir,\n","                 op,\n","                 mask_json_path,\n","                 transforms=None):\n","        \"\"\"\n","        ##TODO: Add support for colorization dataset\n","\n","        Args:\n","            input_dir (str): Path to either colorization or segmentation directory\n","            op (str): One of \"train\", \"val\", or \"test\" signifying the desired split\n","            mask_json_path (str): Path to mapping.json file\n","            transforms (list or None): Image transformations to apply upon loading.\n","        \"\"\"\n","        self.transform = transforms\n","        self.op = op\n","        with open(mask_json_path, 'r') as f:\n","            print(\"f\", f)\n","            print(\"f data\", f.read())\n","            self.mask = json.load(f)\n","            print(self.mask)\n","        self.mask_num = len(self.mask)  # There are 6 categories: grey, dark grey, and black\n","        self.mask_value = [value for value in self.mask.values()]\n","        self.mask_value.sort()\n","        try:\n","            if self.op == 'train':\n","                self.data_dir = os.path.join(input_dir, 'train')\n","            elif self.op == 'val':\n","                self.data_dir = os.path.join(input_dir, 'validation')\n","            elif self.op == 'test':\n","                self.data_dir = os.path.join(input_dir, 'test')\n","        except ValueError:\n","            print('op should be either train, val or test!')\n","\n","    def __len__(self):\n","        \"\"\"\n","\n","        \"\"\"\n","        return len(next(os.walk(self.data_dir))[1])\n","\n","    def __getitem__(self,\n","                    idx):\n","        \"\"\"\n","\n","        \"\"\"\n","        ## Load Image and Parse Properties\n","        img_name = str(idx) + '_input.jpg'\n","        mask_name = str(idx) + '_mask.png'\n","        img = io.imread(os.path.join(self.data_dir, str(idx), img_name))\n","        mask = io.imread(os.path.join(self.data_dir, str(idx), mask_name))\n","        if len(mask.shape) == 2:\n","            h, w  = mask.shape\n","        elif len(mask.shape) == 3:\n","            h, w, c = mask.shape\n","        ## Convert grey-scale label to one-hot encoding\n","        new_mask = np.zeros((h, w, self.mask_num))\n","        for idx in range(self.mask_num):\n","            #if the mask has 3 dimension use this code\n","            new_mask[:, :, idx] = mask[:,:,0] == self.mask_value[idx]\n","            #if the mask has 1 dimension use the code below\n","            #new_mask[:, :, idx] = mask == self.mask_value[idx]\n","        ## Transform image and mask\n","        if self.transform:\n","            img, mask = self.img_transform(img, new_mask)\n","        # ## Use dictionary to output\n","        # sample = {'img': img, 'mask': mask}\n","        # return sample\n","        return img, mask\n","\n","    def img_transform(self,\n","                      img,\n","                      mask):\n","        \"\"\"\n","\n","        \"\"\"\n","        ## Apply Transformations to Image and Mask\n","        img = self.transform(img)\n","        mask = self.transform(mask)\n","        return img, mask"]},{"cell_type":"markdown","metadata":{"id":"OhdKw8V_WDk5"},"source":["## Model Architecture\n","\n","Finish building the U-net architecture below."]},{"cell_type":"code","execution_count":40,"metadata":{"id":"ZM98wgQim9Dj"},"outputs":[],"source":["## Functions for adding the convolution layer\n","def add_conv_stage(dim_in,\n","                   dim_out,\n","                   kernel_size=3,\n","                   stride=1,\n","                   padding=1,\n","                   bias=True,\n","                   useBN=True):\n","    \"\"\"\n","\n","    \"\"\"\n","    # Use batch normalization\n","    if useBN:\n","        return nn.Sequential(\n","          nn.Conv2d(dim_in, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n","          nn.BatchNorm2d(dim_out),\n","          nn.LeakyReLU(0.1),\n","          nn.Conv2d(dim_out, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n","          nn.BatchNorm2d(dim_out),\n","          nn.LeakyReLU(0.1)\n","        )\n","    # No batch normalization\n","    else:\n","        return nn.Sequential(\n","          nn.Conv2d(dim_in, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n","          nn.ReLU(),\n","          nn.Conv2d(dim_out, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n","          nn.ReLU()\n","        )\n","\n","## Upsampling\n","def upsample(ch_coarse,\n","             ch_fine):\n","    \"\"\"\n","\n","    \"\"\"\n","    return nn.Sequential(\n","                    nn.ConvTranspose2d(ch_coarse, ch_fine, 4, 2, 1, bias=False),\n","                    nn.ReLU())\n","\n","\n","# U-Net\n","class UNET(nn.Module):\n","\n","    \"\"\"\n","\n","    \"\"\"\n","    def __init__(self, n_classes, useBN=True):\n","        \"\"\"\n","        Args:\n","            n_classes (int): Number of classes\n","            useBN (bool): Turn Batch Norm on or off. (Hint: Using BatchNorm might help you achieve better performance.)\n","        \"\"\"\n","        super(UNET, self).__init__()\n","        # Downgrade stages\n","        self.conv1 = add_conv_stage(3, 32, useBN=useBN)\n","        self.conv2 = add_conv_stage(32, 64, useBN=useBN)\n","        self.conv3 = add_conv_stage(64, 128, useBN=useBN)\n","        self.conv4 = add_conv_stage(128, 256, useBN=useBN)\n","        # Upgrade stages\n","        self.conv3m = add_conv_stage(256, 128, useBN=useBN)\n","        self.conv2m = add_conv_stage(128,  64, useBN=useBN)\n","        self.conv1m = add_conv_stage( 64,  32, useBN=useBN)\n","        # Maxpool\n","        self.max_pool = nn.MaxPool2d(2)\n","        # Upsample layers\n","        self.upsample43 = upsample(256, 128)\n","        self.upsample32 = upsample(128,  64)\n","        self.upsample21 = upsample(64 ,  32)\n","        # weight initialization\n","        # You can have your own weight intialization. This is just an example.\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","        #TODO: Design your last layer & activations\n","\n","        self.out_conv = add_conv_stage(32, n_classes, kernel_size=1, useBN=False)\n","\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass\n","        \"\"\"\n","        conv1_out = self.conv1(x)\n","        conv2_out = self.conv2(self.max_pool(conv1_out))\n","        conv3_out = self.conv3(self.max_pool(conv2_out))\n","        conv4_out = self.conv4(self.max_pool(conv3_out))\n","\n","        conv4m_out_ = torch.cat((self.upsample43(conv4_out), conv3_out), 1)\n","        conv3m_out  = self.conv3m(conv4m_out_)\n","\n","        conv3m_out_ = torch.cat((self.upsample32(conv3m_out), conv2_out), 1)\n","        conv2m_out  = self.conv2m(conv3m_out_)\n","\n","        conv2m_out_ = torch.cat((self.upsample21(conv2m_out), conv1_out), 1)\n","        conv1m_out  = self.conv1m(conv2m_out_)\n","\n","        #TODO: Design your last layer & activations\n","\n","        out = self.out_conv(conv1m_out)\n","        out = torch.sigmoid(out)\n","\n","        return out\n"]},{"cell_type":"markdown","metadata":{"id":"zn8qJ5y7WDk9"},"source":["### DICE Score and DICE Loss\n","\n","Finish implementing the DICE score function below and then write a Dice Loss function that you can use to update your model weights."]},{"cell_type":"code","execution_count":41,"metadata":{"id":"w4JGCHGrnctJ"},"outputs":[],"source":["##TODO: Finish implementing the multi-class DICE score function\n","def dice_score_image(prediction, target, n_classes):\n","    '''\n","      computer the mean dice score for a single image\n","\n","      Reminders: A false positive is a result that indicates a given condition exists, when it does not\n","               A false negative is a test result that indicates that a condition does not hold, while in fact it does\n","      Args:\n","          prediction (tensor): predictied labels of the image\n","          target (tensor): ground truth of the image\n","          n_classes (int): number of classes\n","\n","      Returns:\n","          m_dice (float): Mean dice score over classes\n","    '''\n","    ## Should test image one by one\n","    assert prediction.shape[0] == 1 #This line can not be deleted\n","    ## TODO: Compute Dice Score for Each Class. Compute Mean Dice Score over Classes.\n","    dice_classes = np.zeros(n_classes)\n","\n","    prediction_one_hot = functional.one_hot(prediction.squeeze(0).to(torch.int64), num_classes=n_classes)  # [256, 320, n_classes]\n","    prediction_one_hot = prediction_one_hot.permute(2, 0, 1).unsqueeze(0)  # [1, n_classes, 256, 320]\n","\n","    for cl in range(n_classes):\n","        pred_flat = prediction_one_hot[:, cl].view(-1).float()\n","        target_flat = target[:, cl].view(-1).float()\n","\n","        TP = (pred_flat * target_flat).sum()\n","        FP = (pred_flat * (1 - target_flat)).sum()\n","        FN = ((1 - pred_flat) * target_flat).sum()\n","\n","        #When there is no ground truth of the class in this image\n","        #Give 1 dice score if False Positive pixel number is 0,\n","        #give 0 dice score if False Positive pixel number is not 0 (> 0).\n","        if target_flat.sum() == 0:\n","            dice_classes[cl] = 1 if FP == 0 else 0\n","        else:\n","            dice_classes[cl] = (2. * TP) / (2. * TP + FP + FN)\n","        \n","    return dice_classes.mean()\n","\n","\n","\n","def dice_score_dataset(model, dataloader, num_classes, use_gpu=True):\n","    \"\"\"\n","    Compute the mean dice score on a set of data.\n","\n","    Note that multiclass dice score can be defined as the mean over classes of binary\n","    dice score. Dice score is computed per image. Mean dice score over the dataset is the dice\n","    score averaged across all images.\n","\n","    Reminders: A false positive is a result that indicates a given condition exists, when it does not\n","               A false negative is a test result that indicates that a condition does not hold, while in fact it does\n","\n","    Args:\n","        model (UNET class): Your trained model\n","        dataloader (DataLoader): Dataset for evaluation\n","        num_classes (int): Number of classes\n","\n","    Returns:\n","        m_dice (float): Mean dice score over the input dataset\n","    \"\"\"\n","    ## Number of Batches and Cache over Dataset\n","    n_batches = len(dataloader)\n","    scores = np.zeros(n_batches)\n","    ## Evaluate\n","    model.eval()\n","    idx = 0\n","    for data in dataloader:\n","        ## Format Data\n","        img, target = data\n","        if use_gpu:\n","            img = img.cuda()\n","            target = target.cuda()\n","        ## Make Predictions\n","        out = model(img)\n","        n_classes = out.shape[1]\n","\n","        prediction = torch.argmax(out, dim = 1)\n","        scores[idx] = dice_score_image(prediction, target, n_classes)\n","        idx += 1\n","    ## Average Dice Score Over Images\n","    m_dice = scores.mean()\n","    return m_dice\n","\n","\n","## TODO: Implement DICE loss,\n","#  It should conform to to how we computer the dice score.\n","class DICELoss(nn.Module):\n","    def __init__(self, num_classes, eps=1e-5):\n","        super(DICELoss, self).__init__()\n","        self.num_classes = num_classes\n","        self.eps = eps\n","\n","    def forward(self, prediction, target):\n","        dice_classes = torch.zeros(self.num_classes, device=prediction.device)\n","\n","        for cl in range(self.num_classes):\n","            pred_cl = prediction[:, cl].view(-1).float()\n","            target_cl = target[:, cl].view(-1).float()\n","\n","            inter = (pred_cl * target_cl).sum()\n","            union = pred_cl.sum() + target_cl.sum() + self.eps\n","            dice_classes[cl] = (2. * inter) / union\n","\n","        dice_loss = 1 - dice_classes.mean()\n","\n","        return dice_loss"]},{"cell_type":"markdown","metadata":{"id":"L5iVvONAWDlB"},"source":["## Training Procedure (Segmentation)"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"Jm_FUG4C0wTl"},"outputs":[{"name":"stdout","output_type":"stream","text":["f <_io.TextIOWrapper name='./HW6_data/mapping.json' mode='r' encoding='UTF-8'>\n","f data {\n","\t\"Background_Tissue\": 0,\n","\t\"Bipolar_Forceps\": 32,\n","\t\"Prograsp_Forceps\": 64,\n","\t\"Large_Needle_Driver\": 96,\n","\t\"Vessel_Sealer\": 128,\n","\t\"Other\": 224\n","}\n"]},{"ename":"JSONDecodeError","evalue":"Expecting value: line 1 column 1 (char 0)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)","\u001b[1;32m/home/kevthatdevs/code/ml-dl/HW6/DeepLearning_Fall2023_HW6.ipynb Cell 18\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kevthatdevs/code/ml-dl/HW6/DeepLearning_Fall2023_HW6.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kevthatdevs/code/ml-dl/HW6/DeepLearning_Fall2023_HW6.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m## Initialize Dataloaders\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kevthatdevs/code/ml-dl/HW6/DeepLearning_Fall2023_HW6.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m train_dataset\u001b[39m=\u001b[39mImageDataset(input_dir\u001b[39m=\u001b[39;49msegmentation_data_dir, op\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m, mask_json_path\u001b[39m=\u001b[39;49mmask_json, transforms\u001b[39m=\u001b[39;49mimg_transform)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kevthatdevs/code/ml-dl/HW6/DeepLearning_Fall2023_HW6.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m validation_dataset\u001b[39m=\u001b[39mImageDataset(input_dir\u001b[39m=\u001b[39msegmentation_data_dir, op\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m, mask_json_path\u001b[39m=\u001b[39mmask_json, transforms\u001b[39m=\u001b[39mimg_transform)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kevthatdevs/code/ml-dl/HW6/DeepLearning_Fall2023_HW6.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m test_dataset\u001b[39m=\u001b[39mImageDataset(input_dir\u001b[39m=\u001b[39msegmentation_data_dir, op\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, mask_json_path\u001b[39m=\u001b[39mmask_json, transforms\u001b[39m=\u001b[39mimg_transform)\n","\u001b[1;32m/home/kevthatdevs/code/ml-dl/HW6/DeepLearning_Fall2023_HW6.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kevthatdevs/code/ml-dl/HW6/DeepLearning_Fall2023_HW6.ipynb#X23sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m, f)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kevthatdevs/code/ml-dl/HW6/DeepLearning_Fall2023_HW6.ipynb#X23sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mf data\u001b[39m\u001b[39m\"\u001b[39m, f\u001b[39m.\u001b[39mread())\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kevthatdevs/code/ml-dl/HW6/DeepLearning_Fall2023_HW6.ipynb#X23sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mload(f)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kevthatdevs/code/ml-dl/HW6/DeepLearning_Fall2023_HW6.ipynb#X23sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kevthatdevs/code/ml-dl/HW6/DeepLearning_Fall2023_HW6.ipynb#X23sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask_num \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask)  \u001b[39m# There are 6 categories: grey, dark grey, and black\u001b[39;00m\n","File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(fp, \u001b[39m*\u001b[39m, \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_float\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_constant\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_pairs_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[39mreturn\u001b[39;00m loads(fp\u001b[39m.\u001b[39;49mread(),\n\u001b[1;32m    294\u001b[0m         \u001b[39mcls\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mcls\u001b[39;49m, object_hook\u001b[39m=\u001b[39;49mobject_hook,\n\u001b[1;32m    295\u001b[0m         parse_float\u001b[39m=\u001b[39;49mparse_float, parse_int\u001b[39m=\u001b[39;49mparse_int,\n\u001b[1;32m    296\u001b[0m         parse_constant\u001b[39m=\u001b[39;49mparse_constant, object_pairs_hook\u001b[39m=\u001b[39;49mobject_pairs_hook, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n","File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n","File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n","File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n","\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"]}],"source":["## Initialize your unet\n","n_classes = 6\n","model = UNET(n_classes)\n","model.to(device)\n","\n","## Initialize Dataloaders\n","train_dataset=ImageDataset(input_dir=segmentation_data_dir, op=\"train\", mask_json_path=mask_json, transforms=img_transform)\n","validation_dataset=ImageDataset(input_dir=segmentation_data_dir, op=\"val\", mask_json_path=mask_json, transforms=img_transform)\n","test_dataset=ImageDataset(input_dir=segmentation_data_dir, op=\"test\", mask_json_path=mask_json, transforms=img_transform)\n","train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n","validation_dataloader = DataLoader(validation_dataset, batch_size=validation_batch_size, shuffle=False)\n","test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n","## Initialize Optimizer and Learning Rate Scheduler\n","optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","criterion = DICELoss(num_classes=n_classes)\n","\n","trainLoss = []\n","valLoss = []\n","\n","print(\"Start Training...\")\n","for epoch in range(num_epochs):\n","    ########################### Training #####################################\n","    print(\"\\nEPOCH \" +str(epoch+1)+\" of \"+str(num_epochs)+\"\\n\")\n","    # TODO: Design your own training section\n","\n","    model.train()\n","    train_loss = 0.0\n","\n","    for input, label in train_dataloader:\n","        input = input.to(device)\n","        label = label.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        output = model(input)\n","        loss = criterion(output, label)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","    \n","    scheduler.step()\n","    train_loss /= len(train_dataloader)\n","    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {train_loss}\")\n","    trainLoss.append(train_loss)\n","\n","    ########################### Validation #####################################\n","    # TODO: Design your own validation section\n","\n","    model.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for input, label in validation_dataloader:\n","            input = input.to(device)\n","            label = label.to(device)\n","\n","            output = model(input)\n","            loss += criterion(output, label).item()\n","\n","        val_loss /= len(validation_dataloader)\n","        print(f\"Epoch {epoch+1}/{num_epochs} - Validation Loss: {val_loss}\")\n","        valLoss.append(val_loss) \n","\n","        test_dice_score = dice_score_dataset(model, test_dataloader, n_classes)\n","        print(f\"Test Dice Score: {test_dice_score}\")\n","\n","with torch.no_grad():\n","    test_dice_score = dice_score_dataset(model, test_dataloader, n_classes)\n","    print(f\"Test Dice Score: {test_dice_score}\")"]},{"cell_type":"markdown","metadata":{"id":"XX67pCavWDlC"},"source":["## Training Procedure: Colorization Pre-training\n","\n","Complete the rest of this problem in the cells below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"geo-2XNCWDlD"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Q72LlQ0cWDlD"},"source":["# Problem 2: Transfer Learning"]},{"cell_type":"markdown","metadata":{"id":"7dkMWrcyWDlD"},"source":["### Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R9MnzH2mWDlD"},"outputs":[],"source":["## Import VGG and FashionMNIST\n","from torchvision.models import vgg16\n","from torchvision.datasets import FashionMNIST"]},{"cell_type":"markdown","metadata":{"id":"LNP3HoCMWDlD"},"source":["### Data Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7NYpDsdPWDlE"},"outputs":[],"source":["## Specify Batch Size\n","train_batch_size = 32\n","test_batch_size = 32\n","\n","## Specify Image Transforms\n","img_transform = transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","## Download Datasets\n","train_data = FashionMNIST('./data', transform=img_transform, download=True, train=True)\n","test_data = FashionMNIST('./data', transform=img_transform, download=True, train=False)\n","\n","## Initialize Dataloaders\n","training_dataloader = DataLoader(train_data, batch_size=train_batch_size, shuffle=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"rq_5k9rCWDlE"},"source":["### Model Initialization and Training/Fine-tuning\n","\n","Complete the rest of the assignment in the notebook below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0nD75TosWDlE"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1WhKIcswOFDaiyzDkJZEx_mZZdOS0LDcg","timestamp":1602791335771}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
