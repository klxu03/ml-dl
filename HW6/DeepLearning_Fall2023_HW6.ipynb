{"cells":[{"cell_type":"markdown","metadata":{"id":"ZQCz6Qbn8Xi3"},"source":["\n","# Deep Learning Homework 6 (Spring 2023)\n","\n","This code is provided for Deep Learning class (601.482/682) Homework 6. For ease of implementation, we recommend working entire in Google Colaboratory.\n","\n","@Copyright Cong Gao, the Johns Hopkins University, cgao11@jhu.edu. Modifications made by Hongtao Wu, Suzanna Sia, Hao Ding, Keith Harrigian, and Yiqing Shen.\n"]},{"cell_type":"markdown","metadata":{"id":"AGA2oroEWDk1"},"source":["### Imports"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"WyxXeYArKAIh"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounting Failed.\n"]}],"source":["## Mount Google Drive Data (If using Google Colaboratory)\n","try:\n","    from google.colab import drive\n","    drive.mount('/content/gdrive')\n","except:\n","    print(\"Mounting Failed.\")"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"diX3FqyIWDk2"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["## Standard Library\n","import os\n","import json\n","\n","## External Libraries\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torchvision import transforms\n","from torch.autograd import Variable\n","import torch.nn.functional as functional\n","from torch.utils.data import Dataset, DataLoader\n","from skimage import io\n","import matplotlib.pyplot as plt\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"Byv0ZVSoWDk2"},"source":["# Problem 1: Unsupervised Pre-training"]},{"cell_type":"markdown","metadata":{"id":"x4NCcExeWDk2"},"source":["### Training Hyperparameters\n","\n","These are recommended hyperparameters - please feel free to use what works for you. Batch size can be changed if it does not match your memory, please state your batch step_size in your report."]},{"cell_type":"markdown","metadata":{"id":"OGb5kRvG1dcD"},"source":["Dataset is available at: https://livejohnshopkins-my.sharepoint.com/:u:/g/personal/yshen92_jh_edu/EcTxWAXsAhtDiv3vUxCTF8gBgAARCUvvKthb3s-pEExyMg"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"sl_S6MdMWDk2"},"outputs":[],"source":["## Batch Size\n","train_batch_size = 10\n","validation_batch_size = 10\n","\n","## Learning Rate\n","learning_rate = 0.001\n","\n","# Epochs (Consider setting high and implementing early stopping)\n","num_epochs = 200"]},{"cell_type":"markdown","metadata":{"id":"m3BRbLhNWDk3"},"source":["### Data Paths"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"aGfa8qhNWDk3"},"outputs":[],"source":["# General Data Directory ##TODO: Please fill in the appropriate directory\n","data_dir = \"./HW6_data\"\n","\n","## Segmentation + Colorization Paths\n","segmentation_data_dir = f\"{data_dir}/segmentation/\"\n","colorization_data_dir = f\"{data_dir}/colorization/\"\n","\n","# Mask JSON\n","mask_json = f\"{data_dir}/mapping.json\""]},{"cell_type":"markdown","metadata":{"id":"gpAXntx2WDk3"},"source":["### Data Loaders\n","\n","We have provided you with some preprocessing code for the images but you should feel free to modify the class however you please to support your training schema. In the very least, you will have to modify the dataloader to support loading of the colorization dataset."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"gL-hqHd1WDk4"},"outputs":[],"source":["## Image Transforms\n","img_transform = transforms.Compose([\n","        transforms.ToTensor(),\n","])\n","\n","## Image Dataloader\n","class ImageDataset(Dataset):\n","\n","    \"\"\"\n","    ImageDataset\n","    \"\"\"\n","\n","    def __init__(self,\n","                 input_dir,\n","                 op,\n","                 mask_json_path,\n","                 transforms=None):\n","        \"\"\"\n","        ##TODO: Add support for colorization dataset\n","\n","        Args:\n","            input_dir (str): Path to either colorization or segmentation directory\n","            op (str): One of \"train\", \"val\", or \"test\" signifying the desired split\n","            mask_json_path (str): Path to mapping.json file\n","            transforms (list or None): Image transformations to apply upon loading.\n","        \"\"\"\n","        self.transform = transforms\n","        self.op = op\n","        with open(mask_json_path, 'r') as f:\n","            self.mask = json.load(f)\n","        self.mask_num = len(self.mask)  # There are 6 categories: grey, dark grey, and black\n","        self.mask_value = [value for value in self.mask.values()]\n","        self.mask_value.sort()\n","        try:\n","            if self.op == 'train':\n","                self.data_dir = os.path.join(input_dir, 'train')\n","            elif self.op == 'val':\n","                self.data_dir = os.path.join(input_dir, 'validation')\n","            elif self.op == 'test':\n","                self.data_dir = os.path.join(input_dir, 'test')\n","        except ValueError:\n","            print('op should be either train, val or test!')\n","\n","    def __len__(self):\n","        \"\"\"\n","\n","        \"\"\"\n","        return len(next(os.walk(self.data_dir))[1])\n","\n","    def __getitem__(self,\n","                    idx):\n","        \"\"\"\n","\n","        \"\"\"\n","        ## Load Image and Parse Properties\n","        img_name = str(idx) + '_input.jpg'\n","        mask_name = str(idx) + '_mask.png'\n","        img = io.imread(os.path.join(self.data_dir, str(idx), img_name))\n","        mask = io.imread(os.path.join(self.data_dir, str(idx), mask_name))\n","        if len(mask.shape) == 2:\n","            h, w  = mask.shape\n","        elif len(mask.shape) == 3:\n","            h, w, c = mask.shape\n","        ## Convert grey-scale label to one-hot encoding\n","        new_mask = np.zeros((h, w, self.mask_num))\n","        for idx in range(self.mask_num):\n","            #if the mask has 3 dimension use this code\n","            new_mask[:, :, idx] = mask[:,:,0] == self.mask_value[idx]\n","            #if the mask has 1 dimension use the code below\n","            #new_mask[:, :, idx] = mask == self.mask_value[idx]\n","        ## Transform image and mask\n","        if self.transform:\n","            img, mask = self.img_transform(img, new_mask)\n","        # ## Use dictionary to output\n","        # sample = {'img': img, 'mask': mask}\n","        # return sample\n","        return img, mask\n","\n","    def img_transform(self,\n","                      img,\n","                      mask):\n","        \"\"\"\n","\n","        \"\"\"\n","        ## Apply Transformations to Image and Mask\n","        img = self.transform(img)\n","        mask = self.transform(mask)\n","        return img, mask"]},{"cell_type":"markdown","metadata":{"id":"OhdKw8V_WDk5"},"source":["## Model Architecture\n","\n","Finish building the U-net architecture below."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"ZM98wgQim9Dj"},"outputs":[],"source":["## Functions for adding the convolution layer\n","def add_conv_stage(dim_in,\n","                   dim_out,\n","                   kernel_size=3,\n","                   stride=1,\n","                   padding=1,\n","                   bias=True,\n","                   useBN=True):\n","    \"\"\"\n","\n","    \"\"\"\n","    # Use batch normalization\n","    if useBN:\n","        return nn.Sequential(\n","          nn.Conv2d(dim_in, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n","          nn.BatchNorm2d(dim_out),\n","          nn.LeakyReLU(0.1),\n","          nn.Conv2d(dim_out, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n","          nn.BatchNorm2d(dim_out),\n","          nn.LeakyReLU(0.1)\n","        )\n","    # No batch normalization\n","    else:\n","        return nn.Sequential(\n","          nn.Conv2d(dim_in, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n","          nn.ReLU(),\n","          nn.Conv2d(dim_out, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n","          nn.ReLU()\n","        )\n","\n","## Upsampling\n","def upsample(ch_coarse,\n","             ch_fine):\n","    \"\"\"\n","\n","    \"\"\"\n","    return nn.Sequential(\n","                    nn.ConvTranspose2d(ch_coarse, ch_fine, 4, 2, 1, bias=False),\n","                    nn.ReLU())\n","\n","\n","# U-Net\n","class UNET(nn.Module):\n","\n","    \"\"\"\n","\n","    \"\"\"\n","    def __init__(self, n_classes, useBN=True):\n","        \"\"\"\n","        Args:\n","            n_classes (int): Number of classes\n","            useBN (bool): Turn Batch Norm on or off. (Hint: Using BatchNorm might help you achieve better performance.)\n","        \"\"\"\n","        super(UNET, self).__init__()\n","        # Downgrade stages\n","        self.conv1 = add_conv_stage(3, 32, useBN=useBN)\n","        self.conv2 = add_conv_stage(32, 64, useBN=useBN)\n","        self.conv3 = add_conv_stage(64, 128, useBN=useBN)\n","        self.conv4 = add_conv_stage(128, 256, useBN=useBN)\n","        # Upgrade stages\n","        self.conv3m = add_conv_stage(256, 128, useBN=useBN)\n","        self.conv2m = add_conv_stage(128,  64, useBN=useBN)\n","        self.conv1m = add_conv_stage( 64,  32, useBN=useBN)\n","        # Maxpool\n","        self.max_pool = nn.MaxPool2d(2)\n","        # Upsample layers\n","        self.upsample43 = upsample(256, 128)\n","        self.upsample32 = upsample(128,  64)\n","        self.upsample21 = upsample(64 ,  32)\n","        # weight initialization\n","        # You can have your own weight intialization. This is just an example.\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","        #TODO: Design your last layer & activations\n","        self.out_conv = nn.Conv2d(32, n_classes, kernel_size=1)\n","\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass\n","        \"\"\"\n","        conv1_out = self.conv1(x)\n","        conv2_out = self.conv2(self.max_pool(conv1_out))\n","        conv3_out = self.conv3(self.max_pool(conv2_out))\n","        conv4_out = self.conv4(self.max_pool(conv3_out))\n","\n","        conv4m_out_ = torch.cat((self.upsample43(conv4_out), conv3_out), 1)\n","        conv3m_out  = self.conv3m(conv4m_out_)\n","\n","        conv3m_out_ = torch.cat((self.upsample32(conv3m_out), conv2_out), 1)\n","        conv2m_out  = self.conv2m(conv3m_out_)\n","\n","        conv2m_out_ = torch.cat((self.upsample21(conv2m_out), conv1_out), 1)\n","        conv1m_out  = self.conv1m(conv2m_out_)\n","\n","        #TODO: Design your last layer & activations\n","\n","        out = self.out_conv(conv1m_out)\n","        out = torch.sigmoid(out)\n","\n","        return out\n"]},{"cell_type":"markdown","metadata":{"id":"zn8qJ5y7WDk9"},"source":["### DICE Score and DICE Loss\n","\n","Finish implementing the DICE score function below and then write a Dice Loss function that you can use to update your model weights."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"w4JGCHGrnctJ"},"outputs":[],"source":["##TODO: Finish implementing the multi-class DICE score function\n","def dice_score_image(prediction, target, n_classes):\n","    '''\n","      computer the mean dice score for a single image\n","\n","      Reminders: A false positive is a result that indicates a given condition exists, when it does not\n","               A false negative is a test result that indicates that a condition does not hold, while in fact it does\n","      Args:\n","          prediction (tensor): predictied labels of the image\n","          target (tensor): ground truth of the image\n","          n_classes (int): number of classes\n","\n","      Returns:\n","          m_dice (float): Mean dice score over classes\n","    '''\n","    ## Should test image one by one\n","    assert prediction.shape[0] == 1 #This line can not be deleted\n","    ## TODO: Compute Dice Score for Each Class. Compute Mean Dice Score over Classes.\n","    dice_classes = np.zeros(n_classes)\n","\n","    prediction_one_hot = functional.one_hot(prediction.squeeze(0).to(torch.int64), num_classes=n_classes)  # [256, 320, n_classes]\n","    prediction_one_hot = prediction_one_hot.permute(2, 0, 1).unsqueeze(0)  # [1, n_classes, 256, 320]\n","\n","    for cl in range(n_classes):\n","        pred_flat = prediction_one_hot[:, cl].view(-1).float()\n","        target_flat = target[:, cl].view(-1).float()\n","\n","        TP = (pred_flat * target_flat).sum()\n","        FP = (pred_flat * (1 - target_flat)).sum()\n","        FN = ((1 - pred_flat) * target_flat).sum()\n","\n","        #When there is no ground truth of the class in this image\n","        #Give 1 dice score if False Positive pixel number is 0,\n","        #give 0 dice score if False Positive pixel number is not 0 (> 0).\n","        if target_flat.sum() == 0:\n","            dice_classes[cl] = 1 if FP == 0 else 0\n","        else:\n","            dice_classes[cl] = (2. * TP) / (2. * TP + FP + FN)\n","        \n","    return dice_classes.mean()\n","\n","\n","\n","def dice_score_dataset(model, dataloader, num_classes, use_gpu=True):\n","    \"\"\"\n","    Compute the mean dice score on a set of data.\n","\n","    Note that multiclass dice score can be defined as the mean over classes of binary\n","    dice score. Dice score is computed per image. Mean dice score over the dataset is the dice\n","    score averaged across all images.\n","\n","    Reminders: A false positive is a result that indicates a given condition exists, when it does not\n","               A false negative is a test result that indicates that a condition does not hold, while in fact it does\n","\n","    Args:\n","        model (UNET class): Your trained model\n","        dataloader (DataLoader): Dataset for evaluation\n","        num_classes (int): Number of classes\n","\n","    Returns:\n","        m_dice (float): Mean dice score over the input dataset\n","    \"\"\"\n","    ## Number of Batches and Cache over Dataset\n","    n_batches = len(dataloader)\n","    scores = np.zeros(n_batches)\n","    ## Evaluate\n","    model.eval()\n","    idx = 0\n","    for data in dataloader:\n","        ## Format Data\n","        img, target = data\n","        if use_gpu:\n","            img = img.cuda()\n","            target = target.cuda()\n","        ## Make Predictions\n","        out = model(img)\n","        n_classes = out.shape[1]\n","\n","        prediction = torch.argmax(out, dim = 1)\n","        scores[idx] = dice_score_image(prediction, target, n_classes)\n","        idx += 1\n","    ## Average Dice Score Over Images\n","    m_dice = scores.mean()\n","    return m_dice\n","\n","\n","## TODO: Implement DICE loss,\n","#  It should conform to to how we computer the dice score.\n","class DICELoss(nn.Module):\n","    def __init__(self, num_classes, eps=1e-5):\n","        super(DICELoss, self).__init__()\n","        self.num_classes = num_classes\n","        self.eps = eps\n","\n","    def forward(self, prediction, target):\n","        dice_classes = torch.zeros(self.num_classes, device=prediction.device)\n","\n","        for cl in range(self.num_classes):\n","            pred_cl = prediction[:, cl, ...].contiguous().view(-1)\n","            target_cl = target[:, cl, ...].contiguous().view(-1)\n","\n","            inter = (pred_cl * target_cl).sum()\n","            union = pred_cl.sum() + target_cl.sum() + self.eps\n","            dice_classes[cl] = (2. * inter) / union\n","\n","        dice_loss = 1 - dice_classes.mean()\n","\n","        return dice_loss"]},{"cell_type":"markdown","metadata":{"id":"L5iVvONAWDlB"},"source":["## Training Procedure (Segmentation)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"Jm_FUG4C0wTl"},"outputs":[{"name":"stdout","output_type":"stream","text":["Start Training...\n","\n","EPOCH 1 of 200\n","\n"]},{"ename":"RuntimeError","evalue":"The size of tensor a (842400) must match the size of tensor b (819200) at non-singleton dimension 0","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m/home/kevthatdevs/code/ml-dl/HW6/DeepLearning_Fall2023_HW6.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kevthatdevs/code/ml-dl/HW6/DeepLearning_Fall2023_HW6.ipynb#X23sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kevthatdevs/code/ml-dl/HW6/DeepLearning_Fall2023_HW6.ipynb#X23sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m output \u001b[39m=\u001b[39m model(\u001b[39minput\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kevthatdevs/code/ml-dl/HW6/DeepLearning_Fall2023_HW6.ipynb#X23sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, label)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kevthatdevs/code/ml-dl/HW6/DeepLearning_Fall2023_HW6.ipynb#X23sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kevthatdevs/code/ml-dl/HW6/DeepLearning_Fall2023_HW6.ipynb#X23sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n","File \u001b[0;32m~/code/ml-dl/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/code/ml-dl/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","\u001b[1;32m/home/kevthatdevs/code/ml-dl/HW6/DeepLearning_Fall2023_HW6.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/kevthatdevs/code/ml-dl/HW6/DeepLearning_Fall2023_HW6.ipynb#X23sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m pred_cl \u001b[39m=\u001b[39m prediction[:, cl, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/kevthatdevs/code/ml-dl/HW6/DeepLearning_Fall2023_HW6.ipynb#X23sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m target_cl \u001b[39m=\u001b[39m target[:, cl, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/kevthatdevs/code/ml-dl/HW6/DeepLearning_Fall2023_HW6.ipynb#X23sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m inter \u001b[39m=\u001b[39m (pred_cl \u001b[39m*\u001b[39;49m target_cl)\u001b[39m.\u001b[39msum()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/kevthatdevs/code/ml-dl/HW6/DeepLearning_Fall2023_HW6.ipynb#X23sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m union \u001b[39m=\u001b[39m pred_cl\u001b[39m.\u001b[39msum() \u001b[39m+\u001b[39m target_cl\u001b[39m.\u001b[39msum() \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meps\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/kevthatdevs/code/ml-dl/HW6/DeepLearning_Fall2023_HW6.ipynb#X23sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m dice_classes[cl] \u001b[39m=\u001b[39m (\u001b[39m2.\u001b[39m \u001b[39m*\u001b[39m inter) \u001b[39m/\u001b[39m union\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (842400) must match the size of tensor b (819200) at non-singleton dimension 0"]}],"source":["## Initialize your unet\n","n_classes = 6\n","model = UNET(n_classes)\n","model.to(device)\n","\n","## Initialize Dataloaders\n","train_dataset=ImageDataset(input_dir=segmentation_data_dir, op=\"train\", mask_json_path=mask_json, transforms=img_transform)\n","validation_dataset=ImageDataset(input_dir=segmentation_data_dir, op=\"val\", mask_json_path=mask_json, transforms=img_transform)\n","test_dataset=ImageDataset(input_dir=segmentation_data_dir, op=\"test\", mask_json_path=mask_json, transforms=img_transform)\n","train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n","validation_dataloader = DataLoader(validation_dataset, batch_size=validation_batch_size, shuffle=False)\n","test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n","## Initialize Optimizer and Learning Rate Scheduler\n","optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","criterion = DICELoss(num_classes=n_classes)\n","\n","trainLoss = []\n","valLoss = []\n","\n","print(\"Start Training...\")\n","for epoch in range(num_epochs):\n","    ########################### Training #####################################\n","    print(\"\\nEPOCH \" +str(epoch+1)+\" of \"+str(num_epochs)+\"\\n\")\n","    # TODO: Design your own training section\n","\n","    model.train()\n","    train_loss = 0.0\n","\n","    for input, label in train_dataloader:\n","        input = input.to(device)\n","        label = label.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        output = model(input)\n","        loss = criterion(output, label)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","    \n","    scheduler.step()\n","    train_loss /= len(train_dataloader)\n","    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {train_loss}\")\n","    trainLoss.append(train_loss)\n","\n","    ########################### Validation #####################################\n","    # TODO: Design your own validation section\n","\n","    model.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for input, label in validation_dataloader:\n","            input = input.to(device)\n","            label = label.to(device)\n","\n","            output = model(input)\n","            loss += criterion(output, label).item()\n","\n","        val_loss /= len(validation_dataloader)\n","        print(f\"Epoch {epoch+1}/{num_epochs} - Validation Loss: {val_loss}\")\n","        valLoss.append(val_loss) \n","\n","        test_dice_score = dice_score_dataset(model, test_dataloader, n_classes)\n","        print(f\"Test Dice Score: {test_dice_score}\")\n","\n","with torch.no_grad():\n","    test_dice_score = dice_score_dataset(model, test_dataloader, n_classes)\n","    print(f\"Test Dice Score: {test_dice_score}\")"]},{"cell_type":"markdown","metadata":{"id":"XX67pCavWDlC"},"source":["## Training Procedure: Colorization Pre-training\n","\n","Complete the rest of this problem in the cells below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"geo-2XNCWDlD"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Q72LlQ0cWDlD"},"source":["# Problem 2: Transfer Learning"]},{"cell_type":"markdown","metadata":{"id":"7dkMWrcyWDlD"},"source":["### Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R9MnzH2mWDlD"},"outputs":[],"source":["## Import VGG and FashionMNIST\n","from torchvision.models import vgg16\n","from torchvision.datasets import FashionMNIST"]},{"cell_type":"markdown","metadata":{"id":"LNP3HoCMWDlD"},"source":["### Data Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7NYpDsdPWDlE"},"outputs":[],"source":["## Specify Batch Size\n","train_batch_size = 32\n","test_batch_size = 32\n","\n","## Specify Image Transforms\n","img_transform = transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","## Download Datasets\n","train_data = FashionMNIST('./data', transform=img_transform, download=True, train=True)\n","test_data = FashionMNIST('./data', transform=img_transform, download=True, train=False)\n","\n","## Initialize Dataloaders\n","training_dataloader = DataLoader(train_data, batch_size=train_batch_size, shuffle=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"rq_5k9rCWDlE"},"source":["### Model Initialization and Training/Fine-tuning\n","\n","Complete the rest of the assignment in the notebook below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0nD75TosWDlE"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1WhKIcswOFDaiyzDkJZEx_mZZdOS0LDcg","timestamp":1602791335771}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
